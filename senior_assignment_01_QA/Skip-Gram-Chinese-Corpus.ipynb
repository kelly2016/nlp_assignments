{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec之Skip-Gram模型实战-中文文本版\n",
    "\n",
    "下面代码将用TensorFlow实现Word2Vec中的Skip-Gram模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用的是爬取的某IT招聘网站的招聘信息训练词向量\n",
    "1. 导包\n",
    "2. 加载数据\n",
    "3. 数据预处理\n",
    "4. 数据采样\n",
    "5. 训练数据构造\n",
    "6. 网络的构建\n",
    "7. 训练\n",
    "8. 可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 加载数据\n",
    "\n",
    "数据集使用的是爬取的某IT招聘网站的招聘信息，数据已经进行切词处理，2944万词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Javasplittedwords') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8898942"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 数据预处理\n",
    "\n",
    "数据预处理过程主要包括：\n",
    "\n",
    "- 替换文本中特殊符号并去除低频词\n",
    "- 对文本分词\n",
    "- 构建语料\n",
    "- 单词映射表\n",
    "\n",
    "首先我们定义一个函数来完成前两步，即对文本的清洗和分词操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数来完成数据的预处理\n",
    "def preprocess(text, freq=50):\n",
    "    '''\n",
    "    对文本进行预处理\n",
    "    \n",
    "    参数\n",
    "    ---\n",
    "    text: 文本数据\n",
    "    freq: 词频阈值\n",
    "    '''\n",
    "    # 对文本中的符号进行替换\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    \n",
    "    # 删除低频词，减少噪音影响\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > freq]\n",
    "\n",
    "    return trimmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的函数实现了替换标点及删除低频词操作，返回分词后的文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面让我们来看看经过清洗后的数据:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['熟练掌握', 'java', '熟悉', 'python', 'shell', '熟练使用', 'git', 'svn', '能够', '发现', '问题', '精准', '定位问题', '快速', '解决问题', '熟悉', 'jvm', 'jvm', '优化', '经验']\n"
     ]
    }
   ],
   "source": [
    "# 清洗文本并分词\n",
    "words = preprocess(text)\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了分词后的文本，就可以构建我们的映射表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建映射表\n",
    "vocab = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6791"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {word: index for index, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_vocab = {index: word for index, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 8623686\n",
      "unique words: 6791\n"
     ]
    }
   ],
   "source": [
    "print(\"total words: {}\".format(len(words)))\n",
    "print(\"unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个文本中单词大约为800万的规模，词典大小为6000左右，这个规模对于训练好的词向量其实是不够的，但可以训练出一个稍微还可以的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对原文本进行vocab到int的转换\n",
    "int_words = [vocab_to_int[w] for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 采样\n",
    "\n",
    "我们知道`skip-gram`中，训练样本的形式是`(input word, output word)`，其中`output word`是`input word`的上下文。为了减少模型噪音并加速训练速度，我们在构造`batch`之前要对样本进行采样，剔除停用词等噪音因素。\n",
    "\n",
    "对停用词进行采样，例如“你”， “我”以及“的”这类单词进行剔除。剔除这些单词以后能够加快我们的训练过程，同时减少训练过程中的噪音。\n",
    "\n",
    "我们采用以下公式:\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
    "\n",
    "其中$ t $是一个阈值参数，一般为1e-3至1e-5。  \n",
    "$f(w_i)$ 是单词 $w_i$ 在整个数据集中的出现频次。  \n",
    "$P(w_i)$ 是单词被删除的概率。\n",
    "\n",
    ">这个公式和论文中描述的那个公式有一些不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_word_counts = Counter(int_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 1e-3 # t值\n",
    "threshold = 0.7 # 剔除概率阈值\n",
    "\n",
    "# 统计单词出现频次\n",
    "int_word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "# 计算单词频率\n",
    "word_freqs = {w: c/total_count for w, c in int_word_counts.items()}\n",
    "# 计算被删除的概率\n",
    "prob_drop = {w: 1 - np.sqrt(t / word_freqs[w]) for w in int_word_counts}\n",
    "# 对单词进行采样\n",
    "train_words = [w for w in int_words if prob_drop[w] < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_words=[int_to_vocab[w] for w in int_words if prob_drop[w] > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'产品', '工作', '开发', '熟悉', '相关', '经验', '能力', '设计', '负责'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(drop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码计算了样本中每个单词被删除的概率，并基于概率进行了采样，现在我们手里就拿到了采样过的单词列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8623686"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7536370"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面数据可以看到，我们本身有800万的文本，经过采样后剩下600万。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 构造batch\n",
    "\n",
    "![](https://pic1.zhimg.com/80/v2-35339b4e3efc29326bad70728e2f469c_hd.png)\n",
    "我们先来分析一下`skip-gram`的样本格式。`skip-gram`不同于`CBOW`，`CBOW`是基于上下文预测当前`input word`。而`skip-gram`则是基于一个`input word`来预测上下文，因此一个input word会对应多个上下文。我们来举个栗子`“[熟练掌握 java 熟悉 python shell 熟练使用 git svn]”`，如果我们固定`skip_window=2`的话，那么`熟悉`的上下文就是`[熟练掌握, java, python, shell]`，如果我们的`batch_size=1`的话，那么实际上一个`batch`中有四个训练样本。\n",
    "\n",
    "上面的分析转换为代码就是两个步骤，第一个是找到每个`input word`的上下文，第二个就是基于上下文构建`batch`。\n",
    "\n",
    "首先是找到`input word`的上下文单词列表：\n",
    "\n",
    "`Skip-Gram`模型是通过输入词来预测上下文。因此我们要构造我们的训练样本。\n",
    "\n",
    "对于一个给定词，离它越近的词可能与它越相关，离它越远的词越不相关，这里我们设置窗口大小为5，对于每个训练单词，我们还会在[1:5]之间随机生成一个整数R，用R作为我们最终选择`output word`的窗口大小。这里之所以多加了一步随机数的窗口重新选择步骤，是为了能够让模型更聚焦于当前`input word`的邻近词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(words, idx, window_size=5):\n",
    "    '''\n",
    "    获得input word的上下文单词列表\n",
    "    \n",
    "    参数\n",
    "    ---\n",
    "    words: 单词列表\n",
    "    idx: input word的索引号\n",
    "    window_size: 窗口大小\n",
    "    '''\n",
    "    target_window = np.random.randint(1, window_size+1)\n",
    "    # 这里要考虑input word前面单词不够的情况\n",
    "    start_point = idx - target_window if (idx - target_window) > 0 else 0\n",
    "    end_point = idx + target_window\n",
    "    # output words(即窗口中的上下文单词)\n",
    "    targets = set(words[start_point: idx] + words[idx+1: end_point+1])\n",
    "    return list(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义了一个`get_targets`函数，接收一个单词索引号，基于这个索引号去查找单词表中对应的上下文（默认window_size=5）。请注意这里有一个小trick，我在实际选择`input word`上下文时，使用的窗口大小是一个介于[1, window_size]区间的随机数。这里的目的是让模型更多地去关注离`input word`更近词。\n",
    "\n",
    "我们有了上面的函数后，就能够轻松地通过`input word`找到它的上下文单词。有了这些单词我们就可以构建我们的`batch`来进行训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    '''\n",
    "    构造一个获取batch的生成器\n",
    "    '''\n",
    "    n_batches = len(words) // batch_size\n",
    "    \n",
    "    # 仅取full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx: idx+batch_size]\n",
    "        for i in range(len(batch)):\n",
    "            batch_x = batch[i]\n",
    "            batch_y = get_targets(batch, i, window_size)\n",
    "            # 由于一个input word会对应多个output word，因此需要长度统一\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "            y.extend(batch_y)\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意上面的代码对`batch`的处理。我们知道对于每个`input word`来说，有多个`output word`（上下文）。例如我们的输入是`熟悉`，上下文是`[熟练掌握, java, python, shell]`，那么`熟悉`这一个batch中就有四个训练样本`[熟悉, 熟练掌握]`, `[熟悉, java]`, `[熟悉, python]`, `[熟悉, shell]`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 构建网络\n",
    "\n",
    "数据预处理结束后，就需要来构建我们的模型。在模型中为了加速训练并提高词向量的质量，我们采用负采样方式进行权重更新。\n",
    "\n",
    "\n",
    "该部分主要包括：\n",
    "\n",
    "- 输入层\n",
    "- Embedding\n",
    "- Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, shape=[None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, shape=[None, None], name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入层\n",
    "\n",
    "输入层到隐层的权重矩阵作为嵌入层要给定其维度，一般embeding_size设置为50-300之间。\n",
    "\n",
    "嵌入矩阵的矩阵形状为 $ vocab\\_size\\times hidden\\_units\\_size$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(int_to_vocab)\n",
    "embedding_size = 300 # 嵌入维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_lookup/Identity:0\", shape=(?, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    # 嵌入层权重矩阵\n",
    "    embedding = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
    "    # 实现lookup\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "    print(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup/Identity:0' shape=(?, 300) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.nn.embedding_lookup函数的用法主要是选取一个张量里面索引对应的元素。tf.nn.embedding_lookup（params, ids）:\n",
    "\n",
    "params可以是张量也可以是数组等，id就是对应的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow中的[tf.nn.embedding_lookup](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup)函数可以实现lookup的计算方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "负采样主要是为了解决梯度下降计算速度慢的问题。\n",
    "\n",
    "TensorFlow中的[tf.nn.sampled_softmax_loss](https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss)会在softmax层上进行采样计算损失，计算出的loss要比full softmax loss低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/roger/.conda/envs/tianchi/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:1124: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    }
   ],
   "source": [
    "n_sampled = 100\n",
    "\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(vocab_size))\n",
    "    \n",
    "    # 计算negative sampling下的损失\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, labels, embed, n_sampled, vocab_size)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意代码中的`softmax_w`的维度是$vocab\\_size * embedding\\_size$，这是因为TensorFlow中的`sampled_softmax_loss`中参数`weights`的`size`是`[num_classes, dim]`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证\n",
    "\n",
    "在上面的步骤中，我们已经将模型的框架搭建出来，下面就让我们来训练训练一下模型。为了能够更加直观地观察训练每个阶段的情况。我们来挑选几个词，看看在训练过程中它们的相似词是怎么变化的。\n",
    "\n",
    "为了更加直观的看到我们训练的结果，我们将查看训练出的相近语义的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "#     # 随机挑选一些单词\n",
    "#     valid_size = 16 \n",
    "#     valid_window = 10\n",
    "#     # 从不同位置各选8个单词\n",
    "#     valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "#     valid_examples = np.append(valid_examples, \n",
    "#                                random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = [vocab_to_int['word'], \n",
    "                      vocab_to_int['北京'], \n",
    "                      vocab_to_int['英语'],\n",
    "                      vocab_to_int['java'], \n",
    "                      vocab_to_int['华为'], \n",
    "                      vocab_to_int['审计'],\n",
    "                      vocab_to_int['健身房'],\n",
    "                      vocab_to_int['学历']]\n",
    "    \n",
    "    valid_size = len(valid_examples)\n",
    "    # 验证单词集\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # 计算每个词向量的模并进行单位化\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keepdims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    # 查找验证单词的词向量\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    # 计算余弦相似度\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7536370"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Iteration: 1000 Avg. Training loss: 2.6856 0.0073 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: 选取, ppt, 周期性, 统统, 各个环节, 电路图, 总公司, 聊天,\n",
      "Nearest to [北京]: 促, 应用于, 引进, 后台运营, 效益, 招募, 达, 互联网产品经理,\n",
      "Nearest to [英语]: 肯定, 转岗, 紧密配合, 优先, 内部人员, 乔布斯, 条件, 红人,\n",
      "Nearest to [java]: 税费, 积极性, shell, 数字, 控制能力, 后端, 链路, 岗位,\n",
      "Nearest to [华为]: 奢侈品, 到来, 爱, 权限, 产品设计, 提醒, 数据库, 共性,\n",
      "Nearest to [审计]: 题, 游戏运营, 其他部门, 健身, 认识, 校园, 新技术, 规避,\n",
      "Nearest to [健身房]: 发展型, 物理, 范围内, 自营, 熟练地, 催, 必须, 组件化,\n",
      "Nearest to [学历]: 为人正直, 挡, 国际贸易, 创业, 所能, 通讯, 转, 美工,\n",
      "****************************************************************************************************\n",
      "Epoch 1/2 Iteration: 2000 Avg. Training loss: 2.0371 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, 周期性, 选取, 各个环节, 敢于创新, 统统, 总公司, 花,\n",
      "Nearest to [北京]: 促, 后台运营, 达, 应用于, 效益, 南京, 用户服务, 引进,\n",
      "Nearest to [英语]: 肯定, 转岗, 紧密配合, 强者, 乔布斯, 红人, 条件, 内部人员,\n",
      "Nearest to [java]: 税费, shell, 积极性, 控制能力, 后端, c++, 数字, 链路,\n",
      "Nearest to [华为]: 奢侈品, 人才培养, 到来, 权限, 爱, 提醒, 共性, 产品设计,\n",
      "Nearest to [审计]: 题, 其他部门, 游戏运营, 健身, 校园, 新技术, 规避, 认识,\n",
      "Nearest to [健身房]: 发展型, 范围内, 自营, 住宿, 突出者, 融入, 催, 必须,\n",
      "Nearest to [学历]: 为人正直, 高中, 国际贸易, 挡, 创业, 35岁, 事件驱动, 通讯,\n",
      "****************************************************************************************************\n",
      "Epoch 1/2 Iteration: 3000 Avg. Training loss: 2.0652 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, 周期性, 选取, 敢于创新, 财务计划, 统统, 各个环节, 图像处理,\n",
      "Nearest to [北京]: 达, 促, 南京, 后台运营, 用户服务, 西路, 控股, 实力,\n",
      "Nearest to [英语]: 肯定, 转岗, 强者, 乔布斯, 条件, 紧密配合, 红人, 优先,\n",
      "Nearest to [java]: 税费, shell, 控制能力, 后端, 积极性, 数字, socket, 链路,\n",
      "Nearest to [华为]: 人才培养, 奢侈品, 分期, 权限, 数据库, 到来, 攻城, 创投,\n",
      "Nearest to [审计]: 题, 渗透, 规避, 其他部门, 游戏运营, 校园, 架构, 健身,\n",
      "Nearest to [健身房]: 发展型, 范围内, 融入, 住宿, 自营, 产品经理, 组件化, 必须,\n",
      "Nearest to [学历]: 为人正直, 国际贸易, 高中, 心理学, 35岁, 挡, 通信, 事件驱动,\n",
      "****************************************************************************************************\n",
      "Epoch 1/2 Iteration: 4000 Avg. Training loss: 1.9189 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, 周期性, 敢于创新, xmind, excel, 选取, 图像处理, 熟练应用,\n",
      "Nearest to [北京]: 西路, 南京, 达, 广州, 控股, 促, 实力, 招募,\n",
      "Nearest to [英语]: 肯定, 强者, 转岗, 乔布斯, 红人, 条件, lua, 小说,\n",
      "Nearest to [java]: shell, 税费, 积极性, 后端, c++, socket, 控制能力, 岗位,\n",
      "Nearest to [华为]: 人才培养, 奢侈品, 分期, 数据库, 攻城, 创投, 权限, 爱,\n",
      "Nearest to [审计]: 题, 规避, 游戏运营, 其他部门, 渗透, 架构, 校园, 管理专业,\n",
      "Nearest to [健身房]: 发展型, 范围内, 应有尽有, 融入, 住宿, 中秋节, 产品经理, 自营,\n",
      "Nearest to [学历]: 国际贸易, 为人正直, 高中, 心理学, 35岁, 挡, 通信, 中专,\n",
      "****************************************************************************************************\n",
      "Epoch 1/2 Iteration: 5000 Avg. Training loss: 1.6721 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, xmind, 敢于创新, 周期性, excel, 常用软件, 图像处理, 思,\n",
      "Nearest to [北京]: 南京, 控股, 广州, 西路, 达, 促, 用户服务, 招募,\n",
      "Nearest to [英语]: 肯定, 强者, 转岗, 红人, 乔布斯, 条件, lua, 行政支持,\n",
      "Nearest to [java]: shell, 税费, 后端, 积极性, c++, 门, socket, 岗位,\n",
      "Nearest to [华为]: 人才培养, 奢侈品, 分期, 创投, 攻城, 到来, 提醒, 职务,\n",
      "Nearest to [审计]: 题, 规避, 其他部门, 渗透, 游戏运营, 架构, 服务水平, 健身,\n",
      "Nearest to [健身房]: 发展型, 范围内, 应有尽有, 产品经理, 到家, 民宿, 融入, 自营,\n",
      "Nearest to [学历]: 国际贸易, 高中, 为人正直, 心理学, 通信, 35岁, 事件驱动, 通讯,\n",
      "****************************************************************************************************\n",
      "Epoch 1/2 Iteration: 6000 Avg. Training loss: 1.7561 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, 敢于创新, 周期性, 财务计划, 常用软件, 图像处理,\n",
      "Nearest to [北京]: 西路, 南京, 控股, 广州, 达, 招募, 促, 高新区,\n",
      "Nearest to [英语]: 肯定, 强者, 转岗, 红人, 乔布斯, 条件, 值得, 行政支持,\n",
      "Nearest to [java]: 后端, shell, 税费, 积极性, 服务器端, 岗位, 门, c++,\n",
      "Nearest to [华为]: 人才培养, 攻城, 奢侈品, 到来, 创投, 提醒, 职务, 分期,\n",
      "Nearest to [审计]: 其他部门, 题, 规避, 渗透, 游戏运营, 行政部, 服务水平, 架构,\n",
      "Nearest to [健身房]: 发展型, 应有尽有, 范围内, 融入, 民宿, 产品经理, 到家, 饮料,\n",
      "Nearest to [学历]: 高中, 国际贸易, 心理学, 为人正直, 中专, 35岁, 挡, 通信,\n",
      "****************************************************************************************************\n",
      "Epoch 1/2 Iteration: 7000 Avg. Training loss: 1.6825 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, 周期性, 敢于创新, 常用软件, 财务计划, 图像处理,\n",
      "Nearest to [北京]: 南京, 西路, 控股, 广州, 达, 高新区, 招募, 促,\n",
      "Nearest to [英语]: 强者, 肯定, 转岗, 红人, 条件, 读写能力, 值得, 乔布斯,\n",
      "Nearest to [java]: 后端, 岗位, 积极性, 税费, shell, 控制能力, 实际, 服务器端,\n",
      "Nearest to [华为]: 人才培养, 攻城, 提醒, 创投, 到来, 职务, 奢侈品, 爱,\n",
      "Nearest to [审计]: 其他部门, 题, 规避, 行政部, 渗透, 游戏运营, 服务水平, 架构,\n",
      "Nearest to [健身房]: 发展型, 应有尽有, 到家, 打球, 民宿, 产品经理, 范围内, 直达,\n",
      "Nearest to [学历]: 国际贸易, 高中, 心理学, 35岁, 事件驱动, 中专, 企业应用, 挡,\n",
      "****************************************************************************************************\n",
      "Epoch 2/2 Iteration: 8000 Avg. Training loss: 1.7186 0.0034 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, 周期性, r, 敢于创新, 财务计划, 思,\n",
      "Nearest to [北京]: 西路, 南京, 控股, 广州, 达, 高新区, 招募, 南山区,\n",
      "Nearest to [英语]: 肯定, 强者, 转岗, 红人, 读写能力, 条件, 乔布斯, 值得,\n",
      "Nearest to [java]: 积极性, 后端, 税费, 岗位, 服务器端, 控制能力, shell, 业,\n",
      "Nearest to [华为]: 人才培养, 创投, 攻城, 奢侈品, 到来, 提醒, 职务, 规模,\n",
      "Nearest to [审计]: 其他部门, 题, 规避, 渗透, 游戏运营, 行政部, 管理专业, 架构,\n",
      "Nearest to [健身房]: 发展型, 应有尽有, 范围内, 到家, 民宿, 打球, 产品经理, 饮料,\n",
      "Nearest to [学历]: 高中, 国际贸易, 35岁, 为人正直, 中专, 挡, 工作效率, 心理学,\n",
      "****************************************************************************************************\n",
      "Epoch 2/2 Iteration: 9000 Avg. Training loss: 1.6746 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, 周期性, 敢于创新, r, 思, 财务计划,\n",
      "Nearest to [北京]: 西路, 南京, 控股, 招募, 达, 广州, 高新区, 促,\n",
      "Nearest to [英语]: 强者, 肯定, 转岗, 红人, 读写能力, 无障碍, 条件, 行政支持,\n",
      "Nearest to [java]: 后端, 积极性, 税费, 岗位, shell, 服务器端, 业, 控制能力,\n",
      "Nearest to [华为]: 人才培养, 攻城, 创投, 到来, 提醒, 奢侈品, 棒, 职务,\n",
      "Nearest to [审计]: 其他部门, 题, 规避, 游戏运营, 行政部, 渗透, 服务水平, 各项任务,\n",
      "Nearest to [健身房]: 发展型, 应有尽有, 打球, 民宿, 饮料, 范围内, 到家, 跑步机,\n",
      "Nearest to [学历]: 高中, 国际贸易, 为人正直, 35岁, 工作效率, 中专, 所能, 通讯,\n",
      "****************************************************************************************************\n",
      "Epoch 2/2 Iteration: 10000 Avg. Training loss: 1.7810 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, 图像处理, 周期性, r, 财务计划, ps,\n",
      "Nearest to [北京]: 西路, 南京, 控股, 招募, 公司名称, 高新区, 达, 广州,\n",
      "Nearest to [英语]: 强者, 肯定, 转岗, 读写能力, 红人, 无障碍, 值得, 行政支持,\n",
      "Nearest to [java]: 后端, 积极性, 税费, 岗位, shell, 服务器端, 控制能力, 语,\n",
      "Nearest to [华为]: 人才培养, 创投, 攻城, 职务, 奢侈品, 分期, 棒, 到来,\n",
      "Nearest to [审计]: 题, 其他部门, 渗透, 规避, 游戏运营, 行政部, 架构, 服务水平,\n",
      "Nearest to [健身房]: 发展型, 应有尽有, 乒乓球, 跑步机, 民宿, 到家, 打球, 范围内,\n",
      "Nearest to [学历]: 高中, 国际贸易, 工作效率, 为人正直, 35岁, 限, 挡, 通讯,\n",
      "****************************************************************************************************\n",
      "Epoch 2/2 Iteration: 11000 Avg. Training loss: 1.8301 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, 图像处理, 周期性, 熟练应用, r, 财务计划,\n",
      "Nearest to [北京]: 西路, 控股, 南京, 招募, 高新区, 公司名称, 广州, 用户服务,\n",
      "Nearest to [英语]: 强者, 肯定, 读写能力, 转岗, 红人, 无障碍, 值得, 条件,\n",
      "Nearest to [java]: 后端, 积极性, 税费, 控制能力, shell, 服务器端, 岗位, 语,\n",
      "Nearest to [华为]: 人才培养, 创投, 攻城, 职务, 奢侈品, 到来, 棒, 哌,\n",
      "Nearest to [审计]: 其他部门, 题, 行政部, 规避, 管理专业, 游戏运营, 渗透, 各项任务,\n",
      "Nearest to [健身房]: 发展型, 应有尽有, 民宿, 跑步机, 乒乓球, 打球, 范围内, 到家,\n",
      "Nearest to [学历]: 高中, 国际贸易, 为人正直, 35岁, 工作效率, 通讯, 中专, 挡,\n",
      "****************************************************************************************************\n",
      "Epoch 2/2 Iteration: 12000 Avg. Training loss: 1.6066 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, 周期性, 图像处理, visio, 熟练应用, r,\n",
      "Nearest to [北京]: 西路, 控股, 南京, 高新区, 公司名称, 广州, 招募, 达,\n",
      "Nearest to [英语]: 强者, 肯定, 转岗, 红人, 读写能力, 无障碍, 值得, 行政支持,\n",
      "Nearest to [java]: 积极性, 后端, 税费, shell, 服务器端, 控制能力, 岗位, 语,\n",
      "Nearest to [华为]: 人才培养, 创投, 攻城, 哌, 职务, 到来, 奢侈品, 棒,\n",
      "Nearest to [审计]: 其他部门, 题, 游戏运营, 渗透, 规避, 行政部, 架构, 各项任务,\n",
      "Nearest to [健身房]: 发展型, 应有尽有, 民宿, 跑步机, 距离, 到家, 乒乓球, 范围内,\n",
      "Nearest to [学历]: 高中, 国际贸易, 工作效率, 35岁, 通讯, 为人正直, 中专, 心理学,\n",
      "****************************************************************************************************\n",
      "Epoch 2/2 Iteration: 13000 Avg. Training loss: 1.5310 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, 图像处理, r, 常用软件, 财务计划, visio,\n",
      "Nearest to [北京]: 西路, 控股, 高新区, 南京, 招募, 公司名称, 广州, 应用于,\n",
      "Nearest to [英语]: 强者, 肯定, 转岗, 无障碍, 红人, 读写能力, 值得, 瑜伽,\n",
      "Nearest to [java]: 后端, 积极性, 税费, 服务器端, shell, 岗位, 语, 控制能力,\n",
      "Nearest to [华为]: 创投, 人才培养, 攻城, 职务, 哌, 到来, 黑, 棒,\n",
      "Nearest to [审计]: 其他部门, 题, 游戏运营, 规避, 渗透, 行政部, 架构, 管理专业,\n",
      "Nearest to [健身房]: 发展型, 民宿, 跑步机, 距离, 应有尽有, 到家, 乒乓球, 范围内,\n",
      "Nearest to [学历]: 高中, 工作效率, 通讯, 国际贸易, 35岁, 为人正直, 中专, 心理学,\n",
      "****************************************************************************************************\n",
      "Epoch 2/2 Iteration: 14000 Avg. Training loss: 1.7581 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, 图像处理, r, 财务计划, 常用软件, visio,\n",
      "Nearest to [北京]: 西路, 南京, 控股, 广州, 招募, 高新区, 公司名称, 应用于,\n",
      "Nearest to [英语]: 强者, 红人, 无障碍, 肯定, 读写能力, 转岗, 英语口语, 瑜伽,\n",
      "Nearest to [java]: 后端, 积极性, 税费, 岗位, 服务器端, 语, shell, 数字,\n",
      "Nearest to [华为]: 人才培养, 创投, 攻城, 哌, 棒, 荣耀, 奢侈品, 到来,\n",
      "Nearest to [审计]: 其他部门, 题, 规避, 行政部, 游戏运营, 管理专业, 架构, 各项任务,\n",
      "Nearest to [健身房]: 发展型, 民宿, 距离, 跑步机, 热水器, 到家, 应有尽有, 乒乓球,\n",
      "Nearest to [学历]: 高中, 工作效率, 国际贸易, 为人正直, 35岁, 限, 通讯, 心理学,\n",
      "****************************************************************************************************\n",
      "Epoch 2/2 Iteration: 15000 Avg. Training loss: 1.5173 0.0072 sec/batch\n",
      "****************************************************************************************************\n",
      "Nearest to [word]: ppt, excel, xmind, r, 图像处理, 周期性, 常用软件, visio,\n",
      "Nearest to [北京]: 西路, 南京, 控股, 公司名称, 广州, 高新区, 招募, 应用于,\n",
      "Nearest to [英语]: 强者, 无障碍, 读写能力, 红人, 肯定, 转岗, 读懂, 英文,\n",
      "Nearest to [java]: 后端, 积极性, 税费, 岗位, 语, 控制能力, 数字, 业,\n",
      "Nearest to [华为]: 人才培养, 创投, 攻城, 乐易, 哌, 黑, 棒, 荣耀,\n",
      "Nearest to [审计]: 其他部门, 题, 规避, 行政部, 游戏运营, 管理专业, 架构, 渗透,\n",
      "Nearest to [健身房]: 发展型, 跑步机, 民宿, 距离, 到家, 应有尽有, 热水器, 乒乓球,\n",
      "Nearest to [学历]: 高中, 国际贸易, 工作效率, 限, 通讯, 35岁, 心理学, 所能,\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "epochs = 2 # 迭代轮数\n",
    "batch_size = 1000 # batch大小\n",
    "window_size = 5 # 窗口大小\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver() # 文件存储\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    # 添加节点用于初始化所有的变量\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        # 获得batch数据\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 1000 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/1000),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/1000))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "           \n",
    "            # 计算相似的词\n",
    "            if iteration % 1000 == 0:\n",
    "                print('*'*100)\n",
    "                # 计算similarity\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_vocab[valid_examples[i]]\n",
    "                    top_k = 8 # 取最相似单词的前8个\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to [%s]:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "                print('*'*100)\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE 是目前来说效果最好的数据降维与可视化方法,可以通过 t-SNE 投影到 2 维或者 3 维的空间中观察一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import font_manager\n",
    "font=font_manager.FontProperties(fname=\"TrueType/simhei.ttf\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 300\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7,fontproperties=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tianchi_python3",
   "language": "python",
   "name": "tianchi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
